{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/code/yovipi/football-predictor-model-ipynb?scriptVersionId=270614351\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2084a",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/code/yovipi/football-predictor-model-ipynb?scriptVersionId=270368239\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91f68b",
   "metadata": {},
   "source": [
    "\n",
    "# The Beautiful Game Oracle â€” Baseline TensorFlow Suite\n",
    "\n",
    "This notebook prepares and compares three TensorFlow/Keras baseline models for predicting English Premier League match outcomes using data pulled directly from the Understat API. It aligns with the project charter in `README.md` and the agent directives in `AGENTS.md`, emphasising reproducible experiments, attribution readiness, and run tracking for longitudinal comparisons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2623a92",
   "metadata": {},
   "source": [
    "\n",
    "## Workflow Overview\n",
    "- Fetch and cache historical EPL match data from Understat for configurable seasons.\n",
    "- Engineer team form, momentum, and market-derived features compatible with TensorFlow pipelines.\n",
    "- Train three complementary baselines (performance-form dense net, momentum interaction network, forecast calibrator) and save artefacts for reuse.\n",
    "- Persist metrics and artefacts per run, append to a cumulative history log, and generate comparison visuals versus prior runs.\n",
    "- Prepare the infrastructure needed for downstream attribution work (Shapley/LOO) by keeping models and datasets aligned with saved run metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e7ee7",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1b520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:24.810624Z",
     "iopub.status.busy": "2025-10-24T19:48:24.810351Z",
     "iopub.status.idle": "2025-10-24T19:48:44.312482Z",
     "shell.execute_reply": "2025-10-24T19:48:44.311355Z",
     "shell.execute_reply.started": "2025-10-24T19:48:24.810604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm.auto import tqdm\n",
    "import xgboost as xgb\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827c5b4",
   "metadata": {},
   "source": [
    "## Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cab39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:44.314625Z",
     "iopub.status.busy": "2025-10-24T19:48:44.314104Z",
     "iopub.status.idle": "2025-10-24T19:48:44.325464Z",
     "shell.execute_reply": "2025-10-24T19:48:44.324648Z",
     "shell.execute_reply.started": "2025-10-24T19:48:44.314601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "PROJECT_NAME = \"The-Beautiful-Game-Oracle\"\n",
    "LEAGUE = \"EPL\"\n",
    "SEASONS = [\"2023\", \"2022\", \"2021\", \"2020\"]  # extend or adjust as needed\n",
    "ROLLING_WINDOW = 5\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "REFRESH_DATA = False  # set True to refetch from Understat\n",
    "RUN_ID = datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
    "DATASET_LABEL = None\n",
    "\n",
    "# Filesystem locations (compatible with Kaggle + local use)\n",
    "if Path(\"/kaggle\").exists():\n",
    "    BASE_WORKING_DIR = Path(\"/kaggle/working\")\n",
    "    LEAGUE_RESULTS_PATH = Path(\"/kaggle/input/Dataset.csv\")\n",
    "else:\n",
    "    BASE_WORKING_DIR = Path(\"./artifacts\")\n",
    "    LEAGUE_RESULTS_PATH = Path(\"understat_data/Dataset.csv\")\n",
    "    if not LEAGUE_RESULTS_PATH.exists():\n",
    "        LEAGUE_RESULTS_PATH = Path(\"understat_data/league_results_v2.csv\")\n",
    "\n",
    "if not LEAGUE_RESULTS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"League results CSV missing at {LEAGUE_RESULTS_PATH}\")\n",
    "\n",
    "EXPERIMENT_ROOT = BASE_WORKING_DIR / \"experiments\"\n",
    "CACHE_DIR = EXPERIMENT_ROOT / \"understat_cache\"\n",
    "MODEL_ARTIFACT_DIR = EXPERIMENT_ROOT / f\"run_{RUN_ID}\"\n",
    "RUN_LOG_PATH = EXPERIMENT_ROOT / \"baseline_run_history.csv\"\n",
    "\n",
    "for path in [EXPERIMENT_ROOT, CACHE_DIR, MODEL_ARTIFACT_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project: {PROJECT_NAME}\")\n",
    "print(f\"Seasons loaded: {SEASONS}\")\n",
    "print(f\"Working dir: {BASE_WORKING_DIR.resolve()}\")\n",
    "print(f\"League results source: {LEAGUE_RESULTS_PATH}\")\n",
    "print(f\"Current run artefacts: {MODEL_ARTIFACT_DIR}\")\n",
    "RUN_LOG_COLUMNS = [\n",
    "    \"timestamp\",\n",
    "    \"run_id\",\n",
    "    \"baseline\",\n",
    "    \"trainer\",\n",
    "    \"feature_view\",\n",
    "    \"train_accuracy\",\n",
    "    \"val_accuracy\",\n",
    "    \"test_accuracy\",\n",
    "    \"train_loss\",\n",
    "    \"val_loss\",\n",
    "    \"test_loss\",\n",
    "    \"val_logloss\",\n",
    "    \"test_logloss\",\n",
    "    \"epochs_trained\",\n",
    "    \"seasons\",\n",
    "    \"dataset_label\",\n",
    "    \"notes\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Label Input\n",
    "Use the box below to assign a dataset label for this run. This label will be stored with training logs and shown in historical comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive dataset label capture\n",
    "dataset_label_box = widgets.Text(\n",
    "    value=DATASET_LABEL or \"\",\n",
    "    placeholder=\"e.g., Kaggle Dataset v1\",\n",
    "    description=\"Dataset:\",\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    ")\n",
    "\n",
    "dataset_label_status = widgets.HTML()\n",
    "\n",
    "def _update_dataset_label(change=None):\n",
    "    global DATASET_LABEL\n",
    "    label = dataset_label_box.value.strip()\n",
    "    DATASET_LABEL = label if label else None\n",
    "    display_value = DATASET_LABEL or \"not set\"\n",
    "    dataset_label_status.value = f\"<b>Active dataset label:</b> {display_value}\"\n",
    "\n",
    "_update_dataset_label()\n",
    "dataset_label_box.observe(_update_dataset_label, names=\"value\")\n",
    "display(widgets.VBox([dataset_label_box, dataset_label_status]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc6855",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3820f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:44.327207Z",
     "iopub.status.busy": "2025-10-24T19:48:44.326771Z",
     "iopub.status.idle": "2025-10-24T19:48:44.400638Z",
     "shell.execute_reply": "2025-10-24T19:48:44.399842Z",
     "shell.execute_reply.started": "2025-10-24T19:48:44.327169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Understat data retrieval helpers\n",
    "\"\"\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; The-Beautiful-Game-Oracle/1.0)\",\n",
    "}\n",
    "\n",
    "MATCHES_PATTERN = re.compile(r\"var\\s+datesData\\s*=\\s*JSON.parse\\('(.+?)'\\)\")\n",
    "\n",
    "\n",
    "def fetch_understat_dates(league: str, season: str, *, refresh: bool = False) -> list:\n",
    "    '''Download Understat league matches for a season, with on-disk caching.'''\n",
    "    cache_path = CACHE_DIR / f\"{league}_{season}_dates.json\"\n",
    "    if cache_path.exists() and not refresh:\n",
    "        try:\n",
    "            return json.loads(cache_path.read_text())\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # fall back to refetching\n",
    "\n",
    "    url = f\"https://understat.com/league/{league}/{season}\"\n",
    "    response = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    match = MATCHES_PATTERN.search(response.text)\n",
    "    if not match:\n",
    "        raise RuntimeError(f\"datesData block not found for {league} {season}\")\n",
    "    decoded = match.group(1).encode(\"utf-8\").decode(\"unicode_escape\")\n",
    "    data = json.loads(decoded)\n",
    "    cache_path.write_text(json.dumps(data))\n",
    "    time.sleep(0.5)  # be gentle with the source\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_matches_for_seasons(league: str, seasons: list[str], refresh: bool = False) -> dict[str, list]:\n",
    "    payload = {}\n",
    "    for season in seasons:\n",
    "        payload[season] = fetch_understat_dates(league, season, refresh=refresh)\n",
    "        print(f\"Loaded {len(payload[season])} fixtures for {league} {season}\")\n",
    "    return payload\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0a709",
   "metadata": {},
   "source": [
    "## Feature Engineering Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84bb6e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:44.402524Z",
     "iopub.status.busy": "2025-10-24T19:48:44.402214Z",
     "iopub.status.idle": "2025-10-24T19:48:44.426794Z",
     "shell.execute_reply": "2025-10-24T19:48:44.425723Z",
     "shell.execute_reply.started": "2025-10-24T19:48:44.402499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Feature engineering utilities\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "CLASS_LABELS = [\"Home Win\", \"Draw\", \"Away Win\"]\n",
    "\n",
    "\n",
    "def resolve_outcome_target(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Map dataset outcome signals to canonical CLASS_LABELS ordering.\"\"\"\n",
    "    label_to_index = {label: idx for idx, label in enumerate(CLASS_LABELS)}\n",
    "    if \"outcome_id\" in df.columns:\n",
    "        outcome_series = pd.to_numeric(df[\"outcome_id\"], errors=\"coerce\")\n",
    "        if \"match_outcome\" in df.columns:\n",
    "            crosswalk = (\n",
    "                pd.DataFrame({\n",
    "                    \"outcome_id\": outcome_series,\n",
    "                    \"match_outcome\": df[\"match_outcome\"],\n",
    "                })\n",
    "                .dropna()\n",
    "                .drop_duplicates()\n",
    "            )\n",
    "            if not crosswalk.empty:\n",
    "                id_to_label = {\n",
    "                    int(row.outcome_id): row.match_outcome\n",
    "                    for row in crosswalk.itertuples(index=False)\n",
    "                    if row.match_outcome in label_to_index\n",
    "                }\n",
    "                if id_to_label:\n",
    "                    remap = {oid: label_to_index[label] for oid, label in id_to_label.items()}\n",
    "                    mapped = outcome_series.map(remap)\n",
    "                    if mapped.notna().all():\n",
    "                        return mapped.astype(int)\n",
    "        canonical_map = {0: label_to_index[\"Home Win\"], 1: label_to_index[\"Draw\"], 2: label_to_index[\"Away Win\"]}\n",
    "        mapped = outcome_series.map(canonical_map)\n",
    "        if mapped.notna().all():\n",
    "            return mapped.astype(int)\n",
    "    if \"match_outcome_code\" in df.columns:\n",
    "        code_map = {\n",
    "            \"H\": label_to_index[\"Home Win\"],\n",
    "            \"D\": label_to_index[\"Draw\"],\n",
    "            \"A\": label_to_index[\"Away Win\"],\n",
    "        }\n",
    "        return df[\"match_outcome_code\"].map(code_map).astype(int)\n",
    "    if \"match_outcome\" in df.columns:\n",
    "        return df[\"match_outcome\"].map(label_to_index).astype(int)\n",
    "    raise KeyError(\"Could not resolve outcome target columns for dataset.\")\n",
    "\n",
    "\n",
    "\n",
    "def build_match_dataframe(matches_by_season: Dict[str, List[dict]]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for season, matches in matches_by_season.items():\n",
    "        for match in matches:\n",
    "            if not match.get(\"isResult\"):\n",
    "                continue  # skip fixtures without a final result\n",
    "            rows.append({\n",
    "                \"match_id\": int(match[\"id\"]),\n",
    "                \"season\": season,\n",
    "                \"match_date\": pd.to_datetime(match[\"datetime\"], format=\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"home_team\": match[\"h\"][\"title\"],\n",
    "                \"away_team\": match[\"a\"][\"title\"],\n",
    "                \"home_goals\": int(match[\"goals\"][\"h\"]),\n",
    "                \"away_goals\": int(match[\"goals\"][\"a\"]),\n",
    "                \"home_xg\": float(match[\"xG\"][\"h\"]),\n",
    "                \"away_xg\": float(match[\"xG\"][\"a\"]),\n",
    "                \"home_prob_win\": float(match[\"forecast\"][\"w\"]),\n",
    "                \"draw_prob\": float(match[\"forecast\"][\"d\"]),\n",
    "                \"away_prob_win\": float(match[\"forecast\"][\"l\"]),\n",
    "            })\n",
    "    frame = pd.DataFrame(rows).sort_values(\"match_date\").reset_index(drop=True)\n",
    "    print(f\"Prepared {len(frame)} completed fixtures across seasons {sorted(matches_by_season.keys())}\")\n",
    "    return frame\n",
    "\n",
    "\n",
    "def build_team_match_rows(match_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for row in match_df.itertuples(index=False):\n",
    "        rows.append({\n",
    "            \"match_id\": row.match_id,\n",
    "            \"season\": row.season,\n",
    "            \"match_date\": row.match_date,\n",
    "            \"team\": row.home_team,\n",
    "            \"opponent\": row.away_team,\n",
    "            \"is_home\": 1,\n",
    "            \"goals_for\": row.home_goals,\n",
    "            \"goals_against\": row.away_goals,\n",
    "            \"xg_for\": row.home_xg,\n",
    "            \"xg_against\": row.away_xg,\n",
    "            \"prob_win\": row.home_prob_win,\n",
    "            \"prob_loss\": row.away_prob_win,\n",
    "            \"draw_prob\": row.draw_prob,\n",
    "        })\n",
    "        rows.append({\n",
    "            \"match_id\": row.match_id,\n",
    "            \"season\": row.season,\n",
    "            \"match_date\": row.match_date,\n",
    "            \"team\": row.away_team,\n",
    "            \"opponent\": row.home_team,\n",
    "            \"is_home\": 0,\n",
    "            \"goals_for\": row.away_goals,\n",
    "            \"goals_against\": row.home_goals,\n",
    "            \"xg_for\": row.away_xg,\n",
    "            \"xg_against\": row.home_xg,\n",
    "            \"prob_win\": row.away_prob_win,\n",
    "            \"prob_loss\": row.home_prob_win,\n",
    "            \"draw_prob\": row.draw_prob,\n",
    "        })\n",
    "    team_df = pd.DataFrame(rows).sort_values(\"match_date\").reset_index(drop=True)\n",
    "    team_df[\"goal_diff\"] = team_df[\"goals_for\"] - team_df[\"goals_against\"]\n",
    "    team_df[\"xg_diff\"] = team_df[\"xg_for\"] - team_df[\"xg_against\"]\n",
    "    return team_df\n",
    "\n",
    "\n",
    "def add_form_features(team_df: pd.DataFrame, window: int = 5) -> pd.DataFrame:\n",
    "    feature_cols = [\n",
    "        \"goals_for\",\n",
    "        \"goals_against\",\n",
    "        \"goal_diff\",\n",
    "        \"xg_for\",\n",
    "        \"xg_against\",\n",
    "        \"xg_diff\",\n",
    "        \"prob_win\",\n",
    "    ]\n",
    "\n",
    "    def enrich(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for col in feature_cols:\n",
    "            shifted = g[col].shift(1)\n",
    "            g[f\"form_{col}_mean\"] = shifted.rolling(window, min_periods=1).mean().fillna(0)\n",
    "            g[f\"form_{col}_std\"] = shifted.rolling(window, min_periods=1).std().fillna(0)\n",
    "            g[f\"form_{col}_last\"] = shifted.fillna(0)\n",
    "        return g\n",
    "\n",
    "    enriched = (\n",
    "        team_df.sort_values(\"match_date\")\n",
    "        .groupby(\"team\", group_keys=False)\n",
    "        .apply(enrich)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return enriched\n",
    "\n",
    "\n",
    "def assemble_match_features(team_form_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    keep_cols = [\"match_id\", \"season\", \"match_date\", \"home_team\", \"away_team\"]\n",
    "\n",
    "    home = team_form_df[team_form_df[\"is_home\"] == 1].copy()\n",
    "    away = team_form_df[team_form_df[\"is_home\"] == 0].copy()\n",
    "\n",
    "    home.rename(columns={\"team\": \"home_team\", \"opponent\": \"away_team\"}, inplace=True)\n",
    "    away.rename(columns={\"team\": \"away_team\", \"opponent\": \"home_team\"}, inplace=True)\n",
    "\n",
    "    def prefix_except(df: pd.DataFrame, prefix: str, exclude: List[str]) -> pd.DataFrame:\n",
    "        rename_map = {col: f\"{prefix}{col}\" for col in df.columns if col not in exclude}\n",
    "        return df.rename(columns=rename_map)\n",
    "\n",
    "    home_prefixed = prefix_except(home, \"home_\", keep_cols)\n",
    "    away_prefixed = prefix_except(away, \"away_\", keep_cols)\n",
    "\n",
    "    merged = home_prefixed.merge(\n",
    "        away_prefixed,\n",
    "        on=[\"match_id\", \"season\", \"match_date\", \"home_team\", \"away_team\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"\", \"_dup\"),\n",
    "    )\n",
    "\n",
    "    if \"home_draw_prob\" in merged.columns:\n",
    "        merged.rename(columns={\"home_draw_prob\": \"match_draw_prob\"}, inplace=True)\n",
    "    if \"away_draw_prob\" in merged.columns:\n",
    "        merged.drop(columns=[\"away_draw_prob\"], inplace=True)\n",
    "\n",
    "    merged[\"target\"] = np.select(\n",
    "        [\n",
    "            merged[\"home_goals_for\"] > merged[\"away_goals_for\"],\n",
    "            merged[\"home_goals_for\"] == merged[\"away_goals_for\"],\n",
    "        ],\n",
    "        [0, 1],\n",
    "        default=2,\n",
    "    )\n",
    "\n",
    "    merged.sort_values(\"match_date\", inplace=True)\n",
    "    merged.reset_index(drop=True, inplace=True)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def build_feature_deltas(match_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = match_df.copy()\n",
    "    df[\"form_goal_diff_delta\"] = df[\"home_form_goal_diff_mean\"] - df[\"away_form_goal_diff_mean\"]\n",
    "    df[\"form_xg_diff_delta\"] = df[\"home_form_xg_diff_mean\"] - df[\"away_form_xg_diff_mean\"]\n",
    "    df[\"form_prob_win_delta\"] = df[\"home_form_prob_win_mean\"] - df[\"away_form_prob_win_mean\"]\n",
    "    df[\"form_goal_last_delta\"] = df[\"home_form_goal_diff_last\"] - df[\"away_form_goal_diff_last\"]\n",
    "    df[\"form_xg_last_delta\"] = df[\"home_form_xg_diff_last\"] - df[\"away_form_xg_diff_last\"]\n",
    "    df[\"prob_edge\"] = df[\"home_prob_win\"] - df[\"away_prob_win\"]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3134b4b",
   "metadata": {},
   "source": [
    "## Dataset Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f426d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:44.428147Z",
     "iopub.status.busy": "2025-10-24T19:48:44.427891Z",
     "iopub.status.idle": "2025-10-24T19:48:45.992495Z",
     "shell.execute_reply": "2025-10-24T19:48:45.991714Z",
     "shell.execute_reply.started": "2025-10-24T19:48:44.428124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load and prepare datasets\n",
    "\n",
    "raw_matches_df = pd.read_csv(\n",
    "    LEAGUE_RESULTS_PATH,\n",
    "    parse_dates=[\"match_datetime_utc\", \"match_date\"],\n",
    ")\n",
    "raw_matches_df[\"is_result\"] = raw_matches_df[\"is_result\"].astype(str).str.lower() == \"true\"\n",
    "\n",
    "# Filter to completed fixtures for the configured league\n",
    "league_filtered_df = raw_matches_df.loc[raw_matches_df[\"league\"] == LEAGUE].copy()\n",
    "match_features_df = (\n",
    "    league_filtered_df.loc[league_filtered_df[\"is_result\"]]\n",
    "    .assign(\n",
    "        season=lambda df: df[\"season\"].astype(str),\n",
    "        match_date=lambda df: pd.to_datetime(df[\"match_date\"], errors=\"coerce\"),\n",
    "        home_team=lambda df: df[\"home_team_name\"],\n",
    "        away_team=lambda df: df[\"away_team_name\"],\n",
    "        home_prob_win=lambda df: df[\"forecast_home_win\"].astype(float),\n",
    "        draw_prob=lambda df: df[\"forecast_draw\"].astype(float),\n",
    "        away_prob_win=lambda df: df[\"forecast_away_win\"].astype(float),\n",
    "        home_prob_loss=lambda df: df[\"forecast_away_win\"].astype(float),\n",
    "        away_prob_loss=lambda df: df[\"forecast_home_win\"].astype(float),\n",
    "        match_draw_prob=lambda df: df[\"forecast_draw\"].astype(float),\n",
    "        prob_edge=lambda df: df[\"forecast_home_win\"].astype(float) - df[\"forecast_away_win\"].astype(float),\n",
    "        match_day_index=lambda df: (df[\"match_date\"] - df[\"match_date\"].min()).dt.days.astype(float),\n",
    "        match_day_of_year=lambda df: df[\"match_date\"].dt.dayofyear.astype(float),\n",
    "        match_day_of_year_norm=lambda df: df[\"match_date\"].dt.dayofyear.astype(float) / 366.0,\n",
    "        match_weekday_index=lambda df: df[\"match_date\"].dt.weekday.astype(float),\n",
    "        match_weekday=lambda df: df[\"match_weekday\"] if \"match_weekday\" in df else df[\"match_date\"].dt.day_name(),\n",
    "        target=lambda df: resolve_outcome_target(df),\n",
    "    )\n",
    "    .sort_values(\"match_datetime_utc\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "weekday_dummies = pd.get_dummies(match_features_df[\"match_weekday\"], prefix=\"match_weekday\", dtype=float)\n",
    "match_features_df = pd.concat([match_features_df, weekday_dummies], axis=1)\n",
    "weekday_cols = [col for col in match_features_df.columns if col.startswith(\"match_weekday_\") and \"match_weekday_index\" not in col]\n",
    "\n",
    "numeric_prefixes = (\n",
    "    \"home_\",\n",
    "    \"away_\",\n",
    "    \"form_\",\n",
    "    \"market_\",\n",
    "    \"rest_\",\n",
    "    \"season_phase\",\n",
    "    \"xg_\",\n",
    "    \"forecast_\",\n",
    "    \"goal_\",\n",
    "    \"total_\",\n",
    "    \"prob_\",\n",
    "    \"match_\",\n",
    "    \"momentum_\",\n",
    ")\n",
    "non_numeric_feature_columns = {\n",
    "    \"home_team\",\n",
    "    \"away_team\",\n",
    "    \"home_team_name\",\n",
    "    \"away_team_name\",\n",
    "    \"home_team_short\",\n",
    "    \"away_team_short\",\n",
    "    \"match_datetime_utc\",\n",
    "    \"match_date\",\n",
    "    \"match_time\",\n",
    "    \"match_weekday\",\n",
    "    \"match_outcome\",\n",
    "    \"match_outcome_code\",\n",
    "}\n",
    "prefixed_cols = [\n",
    "    col\n",
    "    for col in match_features_df.columns\n",
    "    if col.startswith(numeric_prefixes) and col not in non_numeric_feature_columns\n",
    "]\n",
    "match_features_df[prefixed_cols] = match_features_df[prefixed_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "print(\n",
    "    f\"Prepared {len(match_features_df)} completed fixtures from {LEAGUE_RESULTS_PATH.name} spanning seasons {sorted(match_features_df['season'].unique())} for league {LEAGUE}\"\n",
    ")\n",
    "\n",
    "display(match_features_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77861e6b",
   "metadata": {},
   "source": [
    "## Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beeb7c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:45.993604Z",
     "iopub.status.busy": "2025-10-24T19:48:45.993336Z",
     "iopub.status.idle": "2025-10-24T19:48:46.011078Z",
     "shell.execute_reply": "2025-10-24T19:48:46.010309Z",
     "shell.execute_reply.started": "2025-10-24T19:48:45.993577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Chronological train/validation/test split\n",
    "sorted_df = match_features_df.sort_values(\"match_date\").reset_index(drop=True)\n",
    "num_matches = len(sorted_df)\n",
    "train_end = int(num_matches * 0.6)\n",
    "val_end = int(num_matches * 0.8)\n",
    "\n",
    "split_indices = {\n",
    "    \"train\": sorted_df.index[:train_end],\n",
    "    \"val\": sorted_df.index[train_end:val_end],\n",
    "    \"test\": sorted_df.index[val_end:],\n",
    "}\n",
    "\n",
    "print({k: len(v) for k, v in split_indices.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5d2dd",
   "metadata": {},
   "source": [
    "## Feature Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0482c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:46.012148Z",
     "iopub.status.busy": "2025-10-24T19:48:46.011891Z",
     "iopub.status.idle": "2025-10-24T19:48:46.035172Z",
     "shell.execute_reply": "2025-10-24T19:48:46.034322Z",
     "shell.execute_reply.started": "2025-10-24T19:48:46.012125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature sets aligned with the three data perspectives outlined in README.md\n",
    "weekday_cols = [\n",
    "    col\n",
    "    for col in match_features_df.columns\n",
    "    if col.startswith(\"match_weekday_\") and \"match_weekday_index\" not in col\n",
    "]\n",
    "\n",
    "def select_features(df: pd.DataFrame, candidates: list[str]) -> list[str]:\n",
    "    \"\"\"Return candidate columns present in the dataframe, preserving order.\"\"\"\n",
    "    return [col for col in candidates if col in df.columns]\n",
    "\n",
    "def unique(seq: list[str]) -> list[str]:\n",
    "    \"\"\"Deduplicate while preserving order.\"\"\"\n",
    "    seen = set()\n",
    "    out: list[str] = []\n",
    "    for item in seq:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            out.append(item)\n",
    "    return out\n",
    "\n",
    "PERFORMANCE_FEATURES = unique(select_features(match_features_df, [\n",
    "    \"home_goals_for_last_5\",\n",
    "    \"home_goals_against_last_5\",\n",
    "    \"home_goal_diff_last_5\",\n",
    "    \"home_xg_for_last_5\",\n",
    "    \"home_xg_against_last_5\",\n",
    "    \"home_xg_diff_last_5\",\n",
    "    \"home_points_last_5\",\n",
    "    \"away_goals_for_last_5\",\n",
    "    \"away_goals_against_last_5\",\n",
    "    \"away_goal_diff_last_5\",\n",
    "    \"away_xg_for_last_5\",\n",
    "    \"away_xg_against_last_5\",\n",
    "    \"away_xg_diff_last_5\",\n",
    "    \"away_points_last_5\",\n",
    "    \"form_diff_last5\",\n",
    "    \"xg_diff_last5\",\n",
    "    \"match_day_index\",\n",
    "    \"match_day_of_year_norm\",\n",
    "    \"match_weekday_index\",\n",
    "]) + weekday_cols)\n",
    "\n",
    "MOMENTUM_FEATURES = unique(select_features(match_features_df, [\n",
    "    \"momentum_points_last3_delta_season_z\",\n",
    "    \"momentum_points_last2_delta_season_z\",\n",
    "    \"momentum_points_last8_delta_season_z\",\n",
    "    \"momentum_points_pct_last3_delta_season_z\",\n",
    "    \"momentum_goal_diff_last3_delta_season_z\",\n",
    "    \"momentum_goal_diff_last2_delta_season_z\",\n",
    "    \"momentum_goal_diff_last8_delta_season_z\",\n",
    "    \"momentum_xg_diff_last3_delta_season_z\",\n",
    "    \"momentum_xg_diff_last2_delta_season_z\",\n",
    "    \"momentum_xg_diff_last8_delta_season_z\",\n",
    "    \"momentum_points_exp_decay_delta_season_z\",\n",
    "    \"momentum_xg_exp_decay_delta_season_z\",\n",
    "    \"momentum_matches_last14_delta_season_z\",\n",
    "    \"momentum_travel_rest_ratio_delta_season_z\",\n",
    "    \"momentum_forecast_win_prev_delta_season_z\",\n",
    "    \"momentum_forecast_trend_delta_season_z\",\n",
    "    \"form_pct_diff_last5_season_z\",\n",
    "    \"form_diff_last5_season_z\",\n",
    "    \"rest_diff_season_z\",\n",
    "    \"fixture_congestion_flag_pair\",\n",
    "    \"momentum_fixture_congestion_delta\",\n",
    "    \"rest_reset_flag_pair\",\n",
    "    \"match_day_index_season_z\",\n",
    "    \"match_day_of_year_norm_season_z\",\n",
    "    \"match_weekday_index_season_z\",\n",
    "]) + weekday_cols)\n",
    "\n",
    "MARKET_FEATURES = unique(select_features(match_features_df, [\n",
    "    \"forecast_home_win\",\n",
    "    \"forecast_draw\",\n",
    "    \"forecast_away_win\",\n",
    "    \"market_home_edge\",\n",
    "    \"market_expected_points_home\",\n",
    "    \"market_expected_points_away\",\n",
    "    \"market_entropy\",\n",
    "    \"market_logit_home\",\n",
    "    \"market_max_prob\",\n",
    "    \"match_day_index\",\n",
    "    \"match_day_of_year_norm\",\n",
    "    \"match_weekday_index\",\n",
    "]) + weekday_cols)\n",
    "\n",
    "FEATURE_SETS = {\n",
    "    \"performance_dense\": {\n",
    "        \"description\": \"Performance-based dense network using rolling xG/goals aggregates\",\n",
    "        \"feature_cols\": PERFORMANCE_FEATURES,\n",
    "        \"trainer\": \"keras\",\n",
    "        \"builder_name\": \"build_performance_model\",\n",
    "    },\n",
    "    \"momentum_policy_rl\": {\n",
    "        \"description\": \"Momentum-policy REINFORCE agent leveraging short-horizon trends\",\n",
    "        \"feature_cols\": MOMENTUM_FEATURES,\n",
    "        \"trainer\": \"reinforce\",\n",
    "        \"policy_hidden_units\": [128, 64],\n",
    "        \"gamma\": 0.95,\n",
    "        \"learning_rate\": 7e-4,\n",
    "        \"policy_epochs\": max(EPOCHS, 60),\n",
    "    },\n",
    "    \"market_gradient_boost\": {\n",
    "        \"description\": \"Market odds derived statistics\",\n",
    "        \"feature_cols\": MARKET_FEATURES,\n",
    "        \"trainer\": \"xgboost\",\n",
    "        \"xgb_params\": {\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": 4,\n",
    "            \"subsample\": 0.85,\n",
    "            \"colsample_bytree\": 0.75,\n",
    "            \"reg_lambda\": 1.0,\n",
    "            \"n_estimators\": 600,\n",
    "            \"min_child_weight\": 2,\n",
    "            \"random_state\": SEED,\n",
    "            \"eval_metric\": \"mlogloss\",\n",
    "            \"tree_method\": \"hist\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "all_feature_columns = sorted({col for spec in FEATURE_SETS.values() for col in spec[\"feature_cols\"]})\n",
    "missing_columns = [col for col in all_feature_columns if col not in match_features_df.columns]\n",
    "if missing_columns:\n",
    "    raise KeyError(f\"Dataset is missing expected feature columns: {missing_columns}\")\n",
    "match_features_df[all_feature_columns] = match_features_df[all_feature_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "print({name: len(spec[\"feature_cols\"]) for name, spec in FEATURE_SETS.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8548b",
   "metadata": {},
   "source": [
    "## TensorFlow Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa8a0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:46.036242Z",
     "iopub.status.busy": "2025-10-24T19:48:46.036007Z",
     "iopub.status.idle": "2025-10-24T19:48:46.060993Z",
     "shell.execute_reply": "2025-10-24T19:48:46.059982Z",
     "shell.execute_reply.started": "2025-10-24T19:48:46.036223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Dataset constructors\n",
    "\n",
    "def build_xy(df: pd.DataFrame, feature_cols: list[str], indices: pd.Index) -> tuple[np.ndarray, np.ndarray]:\n",
    "    X = df.loc[indices, feature_cols].astype(np.float32).to_numpy()\n",
    "    y = df.loc[indices, \"target\"].astype(np.int32).to_numpy()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_dataset(X: np.ndarray, y: np.ndarray, *, training: bool) -> tf.data.Dataset:\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=len(X), seed=SEED, reshuffle_each_iteration=True)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "def compute_class_weights(y: np.ndarray) -> dict[int, float]:\n",
    "    counts = np.bincount(y, minlength=len(CLASS_LABELS))\n",
    "    total = float(len(y))\n",
    "    weights = {cls: total / (len(CLASS_LABELS) * count) for cls, count in enumerate(counts) if count > 0}\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae37026",
   "metadata": {},
   "source": [
    "## Baseline Model Builders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4594c2",
   "metadata": {},
   "source": [
    "### Performance Dense Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62120879",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:46.062134Z",
     "iopub.status.busy": "2025-10-24T19:48:46.061885Z",
     "iopub.status.idle": "2025-10-24T19:48:46.084187Z",
     "shell.execute_reply": "2025-10-24T19:48:46.082945Z",
     "shell.execute_reply.started": "2025-10-24T19:48:46.062114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_performance_model(input_dim: int) -> keras.Model:\n",
    "    inputs = keras.Input(shape=(input_dim,), name=\"performance_features\")\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "    x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dropout(0.15)(x)\n",
    "    outputs = keras.layers.Dense(len(CLASS_LABELS), activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"performance_dense\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32a102",
   "metadata": {},
   "source": [
    "### Momentum Interaction Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9bf86f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:46.087575Z",
     "iopub.status.busy": "2025-10-24T19:48:46.087189Z",
     "iopub.status.idle": "2025-10-24T19:48:46.105119Z",
     "shell.execute_reply": "2025-10-24T19:48:46.104134Z",
     "shell.execute_reply.started": "2025-10-24T19:48:46.087547Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_policy_network(input_dim: int, hidden_units: list[int]) -> keras.Model:\n",
    "    inputs = keras.Input(shape=(input_dim,), name=\"policy_state\")\n",
    "    x = inputs\n",
    "    for units in hidden_units:\n",
    "        x = keras.layers.Dense(units, activation=\"relu\")(x)\n",
    "        x = keras.layers.LayerNormalization()(x)\n",
    "        x = keras.layers.Dropout(0.2)(x)\n",
    "    outputs = keras.layers.Dense(len(CLASS_LABELS), activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs, name=\"momentum_policy\")\n",
    "\n",
    "\n",
    "def discount_returns(rewards: tf.Tensor, gamma: float) -> tf.Tensor:\n",
    "    rewards_np = rewards.numpy() if isinstance(rewards, tf.Tensor) else np.asarray(rewards, dtype=np.float32)\n",
    "    discounted = np.zeros_like(rewards_np, dtype=np.float32)\n",
    "    running = 0.0\n",
    "    for t in range(len(rewards_np) - 1, -1, -1):\n",
    "        running = rewards_np[t] + gamma * running\n",
    "        discounted[t] = running\n",
    "    if discounted.std() > 1e-6:\n",
    "        discounted = (discounted - discounted.mean()) / (discounted.std() + 1e-6)\n",
    "    else:\n",
    "        discounted = discounted - discounted.mean()\n",
    "    return tf.convert_to_tensor(discounted, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PolicyGradientAgent:\n",
    "    \"\"\"Simple REINFORCE agent for sequential match prediction.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        *,\n",
    "        hidden_units: list[int],\n",
    "        learning_rate: float,\n",
    "        gamma: float,\n",
    "    ) -> None:\n",
    "        self.model = build_policy_network(input_dim, hidden_units)\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.model.predict(X, verbose=0)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_episodes: list[tuple[np.ndarray, np.ndarray]],\n",
    "        val_data: tuple[np.ndarray, np.ndarray],\n",
    "        *,\n",
    "        epochs: int,\n",
    "        eval_interval: int = 1,\n",
    "        progress_desc: str | None = None,\n",
    "    ) -> dict[str, list[float]]:\n",
    "        history = {\"policy_loss\": [], \"avg_reward\": [], \"val_accuracy\": []}\n",
    "        X_val, y_val = val_data\n",
    "        best_weights = [np.copy(w) for w in self.model.get_weights()]\n",
    "        best_val_acc = -np.inf\n",
    "\n",
    "        progress = tqdm(range(epochs), desc=progress_desc or \"Policy training\", leave=False)\n",
    "        for epoch in progress:\n",
    "            batch_losses = []\n",
    "            batch_rewards = []\n",
    "            for states, actions in train_episodes:\n",
    "                states_tf = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "                actions_tf = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    probs = self.model(states_tf, training=True)\n",
    "                    sampled_actions = tf.random.categorical(\n",
    "                        tf.math.log(probs + 1e-9), num_samples=1, dtype=actions_tf.dtype\n",
    "                    )\n",
    "                    sampled_actions = tf.squeeze(sampled_actions, axis=1)\n",
    "                    batch_indices = tf.range(\n",
    "                        tf.shape(sampled_actions)[0], dtype=sampled_actions.dtype\n",
    "                    )\n",
    "                    gather_idx = tf.stack([batch_indices, sampled_actions], axis=1)\n",
    "                    chosen_probs = tf.gather_nd(probs, gather_idx)\n",
    "                    rewards = tf.cast(tf.equal(sampled_actions, actions_tf), tf.float32)\n",
    "                    returns = discount_returns(rewards, self.gamma)\n",
    "                    loss = -tf.reduce_mean(tf.math.log(chosen_probs + 1e-9) * returns)\n",
    "                grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "                batch_losses.append(float(loss.numpy()))\n",
    "                batch_rewards.append(float(tf.reduce_mean(rewards).numpy()))\n",
    "\n",
    "            history[\"policy_loss\"].append(float(np.mean(batch_losses)))\n",
    "            history[\"avg_reward\"].append(float(np.mean(batch_rewards)))\n",
    "\n",
    "            if (epoch + 1) % eval_interval == 0:\n",
    "                val_probs = self.predict_proba(X_val)\n",
    "                val_preds = np.argmax(val_probs, axis=1)\n",
    "                val_acc = float(np.mean(val_preds == y_val))\n",
    "                history[\"val_accuracy\"].append(val_acc)\n",
    "                progress.set_postfix({\"val_acc\": f\"{val_acc:.3f}\", \"loss\": f\"{history['policy_loss'][-1]:.3f}\"})\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_weights = [np.copy(w) for w in self.model.get_weights()]\n",
    "\n",
    "        if best_weights is not None:\n",
    "            self.model.set_weights(best_weights)\n",
    "        progress.close()\n",
    "\n",
    "        return history\n",
    "\n",
    "    def save(self, path: Path) -> None:\n",
    "        self.model.save(path, include_optimizer=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6e9e3",
   "metadata": {},
   "source": [
    "### Forecast Calibrator Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d838b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:46.106232Z",
     "iopub.status.busy": "2025-10-24T19:48:46.106003Z",
     "iopub.status.idle": "2025-10-24T19:48:46.141453Z",
     "shell.execute_reply": "2025-10-24T19:48:46.140436Z",
     "shell.execute_reply.started": "2025-10-24T19:48:46.106214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_episodes(df: pd.DataFrame, feature_cols: list[str], indices: pd.Index) -> list[tuple[np.ndarray, np.ndarray]]:\n",
    "    subset = (\n",
    "        df.loc[indices, [\"season\", \"match_date\", \"target\", *feature_cols]]\n",
    "        .sort_values([\"season\", \"match_date\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    episodes: list[tuple[np.ndarray, np.ndarray]] = []\n",
    "    for _, season_df in subset.groupby(\"season\", sort=False):\n",
    "        states = season_df[feature_cols].astype(np.float32).to_numpy()\n",
    "        actions = season_df[\"target\"].astype(np.int32).to_numpy()\n",
    "        episodes.append((states, actions))\n",
    "    return episodes\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, probs: np.ndarray) -> tuple[float, float, np.ndarray]:\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    accuracy = float(np.mean(preds == y_true))\n",
    "    clipped = np.clip(probs, 1e-8, 1.0 - 1e-8)\n",
    "    logloss = float(-np.mean(np.log(clipped[np.arange(len(y_true)), y_true])))\n",
    "    return accuracy, logloss, preds\n",
    "\n",
    "\n",
    "def save_confusion_heatmap(cm: np.ndarray, labels: list[str], path: Path, title: str) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(labels)\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            ax.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(title)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=160)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "def to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_serializable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [to_serializable(v) for v in obj]\n",
    "    if isinstance(obj, (np.floating, np.integer)):\n",
    "        return obj.item()\n",
    "    return obj\n",
    "\n",
    "class TQDMKerasCallback(keras.callbacks.Callback):\n",
    "    \"\"\"Lightweight tqdm progress bar for Keras epoch loops.\"\"\"\n",
    "\n",
    "    def __init__(self, total_epochs: int, desc: str) -> None:\n",
    "        super().__init__()\n",
    "        self.total_epochs = total_epochs\n",
    "        self.desc = desc\n",
    "        self._pbar = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self._pbar = tqdm(total=self.total_epochs, desc=self.desc, leave=False)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self._pbar is not None:\n",
    "            self._pbar.update(1)\n",
    "            if logs and \"val_accuracy\" in logs:\n",
    "                self._pbar.set_postfix({\"val_acc\": f\"{logs['val_accuracy']:.3f}\", \"loss\": f\"{logs.get('loss', 0.0):.3f}\"})\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self._pbar is not None:\n",
    "            self._pbar.close()\n",
    "            self._pbar = None\n",
    "\n",
    "def load_run_log(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame(columns=RUN_LOG_COLUMNS)\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except pd.errors.ParserError:\n",
    "        df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    # Align with canonical column order\n",
    "    df = df.reindex(columns=RUN_LOG_COLUMNS)\n",
    "    if \"dataset_label\" not in df.columns:\n",
    "        df[\"dataset_label\"] = \"\"\n",
    "    df[\"dataset_label\"] = df[\"dataset_label\"].fillna(\"\").astype(str)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb0f997",
   "metadata": {},
   "source": [
    "### Baseline Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5644dda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:46.142645Z",
     "iopub.status.busy": "2025-10-24T19:48:46.142404Z",
     "iopub.status.idle": "2025-10-24T19:48:46.159633Z",
     "shell.execute_reply": "2025-10-24T19:48:46.158734Z",
     "shell.execute_reply.started": "2025-10-24T19:48:46.142626Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASELINE_NAMES = list(FEATURE_SETS.keys())\n",
    "print(\"Baselines configured:\", BASELINE_NAMES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd01ac",
   "metadata": {},
   "source": [
    "## Training and Run Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e1353c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:48:46.161359Z",
     "iopub.status.busy": "2025-10-24T19:48:46.160762Z",
     "iopub.status.idle": "2025-10-24T19:49:04.355079Z",
     "shell.execute_reply": "2025-10-24T19:49:04.354011Z",
     "shell.execute_reply.started": "2025-10-24T19:48:46.161328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training loop with experiment logging\n",
    "sce = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "baseline_results = []\n",
    "run_log_entries = []\n",
    "\n",
    "baseline_items = list(FEATURE_SETS.items())\n",
    "\n",
    "for baseline_name, spec in tqdm(baseline_items, desc=\"Baselines\", leave=False):\n",
    "    trainer = spec.get(\"trainer\", \"keras\")\n",
    "    feature_cols = spec[\"feature_cols\"]\n",
    "    print(f\"Training baseline: {baseline_name} ({trainer})\")\n",
    "\n",
    "    X_train, y_train = build_xy(sorted_df, feature_cols, split_indices[\"train\"])\n",
    "    X_val, y_val = build_xy(sorted_df, feature_cols, split_indices[\"val\"])\n",
    "    X_test, y_test = build_xy(sorted_df, feature_cols, split_indices[\"test\"])\n",
    "\n",
    "    baseline_dir = MODEL_ARTIFACT_DIR / baseline_name\n",
    "    baseline_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    history_dict: dict[str, list] = {}\n",
    "    epochs_trained = 0\n",
    "    class_weights = None\n",
    "\n",
    "    if trainer == \"keras\":\n",
    "        builder_name = spec.get(\"builder_name\")\n",
    "        if builder_name is None:\n",
    "            raise ValueError(f\"No builder_name provided for {baseline_name}\")\n",
    "        builder = globals()[builder_name]\n",
    "\n",
    "        train_ds = make_dataset(X_train, y_train, training=True)\n",
    "        val_ds = make_dataset(X_val, y_val, training=False)\n",
    "        test_ds = make_dataset(X_test, y_test, training=False)\n",
    "        train_eval_ds = make_dataset(X_train, y_train, training=False)\n",
    "\n",
    "        class_weights = compute_class_weights(y_train)\n",
    "\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5),\n",
    "            TQDMKerasCallback(EPOCHS, f\"{baseline_name} epochs\"),\n",
    "        ]\n",
    "\n",
    "        model = builder(len(feature_cols))\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights,\n",
    "            verbose=0,\n",
    "        )\n",
    "        history_dict = history.history\n",
    "        epochs_trained = len(history_dict.get(\"loss\", []))\n",
    "\n",
    "        train_probs = model.predict(train_eval_ds, verbose=0)\n",
    "        val_probs = model.predict(val_ds, verbose=0)\n",
    "        test_probs = model.predict(test_ds, verbose=0)\n",
    "\n",
    "        model.save(baseline_dir / \"model.keras\", include_optimizer=False)\n",
    "\n",
    "    elif trainer == \"reinforce\":\n",
    "        input_dim = X_train.shape[1]\n",
    "        train_episodes = build_episodes(sorted_df, feature_cols, split_indices[\"train\"])\n",
    "        agent = PolicyGradientAgent(\n",
    "            input_dim,\n",
    "            hidden_units=spec.get(\"policy_hidden_units\", [128, 64]),\n",
    "            learning_rate=spec.get(\"learning_rate\", 5e-4),\n",
    "            gamma=spec.get(\"gamma\", 0.95),\n",
    "        )\n",
    "        policy_epochs = spec.get(\"policy_epochs\", max(EPOCHS, 60))\n",
    "        history_dict = agent.train(\n",
    "            train_episodes,\n",
    "            val_data=(X_val, y_val),\n",
    "            epochs=policy_epochs,\n",
    "            eval_interval=spec.get(\"eval_interval\", 1),\n",
    "            progress_desc=f\"{baseline_name} policy\",\n",
    "        )\n",
    "        epochs_trained = len(history_dict.get(\"policy_loss\", []))\n",
    "\n",
    "        train_probs = agent.predict_proba(X_train)\n",
    "        val_probs = agent.predict_proba(X_val)\n",
    "        test_probs = agent.predict_proba(X_test)\n",
    "\n",
    "        agent.save(baseline_dir / \"model.keras\")\n",
    "\n",
    "    elif trainer == \"xgboost\":\n",
    "        params = spec.get(\"xgb_params\", {}).copy()\n",
    "        eval_metric = params.pop(\"eval_metric\", \"mlogloss\")\n",
    "        model = xgb.XGBClassifier(eval_metric=eval_metric, **params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=40,\n",
    "        )\n",
    "        history_dict = model.evals_result()\n",
    "        if hasattr(model, \"best_iteration\") and model.best_iteration is not None:\n",
    "            epochs_trained = int(model.best_iteration) + 1\n",
    "        else:\n",
    "            epochs_trained = int(model.n_estimators)\n",
    "\n",
    "        train_probs = model.predict_proba(X_train)\n",
    "        val_probs = model.predict_proba(X_val)\n",
    "        test_probs = model.predict_proba(X_test)\n",
    "\n",
    "        model.save_model(str(baseline_dir / \"model.json\"))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown trainer type: {trainer}\")\n",
    "\n",
    "    train_accuracy, train_logloss, train_preds = compute_metrics(y_train, train_probs)\n",
    "    val_accuracy, val_logloss, val_preds = compute_metrics(y_val, val_probs)\n",
    "    test_accuracy, test_logloss, test_preds = compute_metrics(y_test, test_probs)\n",
    "\n",
    "    confusion = tf.math.confusion_matrix(y_test, test_preds, num_classes=len(CLASS_LABELS)).numpy()\n",
    "    save_confusion_heatmap(confusion, CLASS_LABELS, baseline_dir / \"confusion_matrix.png\", f\"{baseline_name} â€” Test Confusion\")\n",
    "\n",
    "    eval_rows = sorted_df.loc[split_indices[\"test\"], [\n",
    "        \"match_id\",\n",
    "        \"season\",\n",
    "        \"match_date\",\n",
    "        \"home_team\",\n",
    "        \"away_team\",\n",
    "        \"target\",\n",
    "    ]].copy()\n",
    "    eval_rows[\"predicted\"] = test_preds\n",
    "    eval_rows[\"predicted_label\"] = eval_rows[\"predicted\"].map(dict(enumerate(CLASS_LABELS)))\n",
    "    eval_rows[\"target_label\"] = eval_rows[\"target\"].map(dict(enumerate(CLASS_LABELS)))\n",
    "    eval_rows[\"home_prob\"] = test_probs[:, 0]\n",
    "    eval_rows[\"draw_prob\"] = test_probs[:, 1]\n",
    "    eval_rows[\"away_prob\"] = test_probs[:, 2]\n",
    "    eval_rows.to_csv(baseline_dir / \"test_predictions.csv\", index=False)\n",
    "\n",
    "    with open(baseline_dir / \"history.json\", \"w\") as fp:\n",
    "        json.dump(to_serializable(history_dict), fp, indent=2)\n",
    "\n",
    "    metrics_payload = {\n",
    "        \"trainer\": trainer,\n",
    "        \"train\": {\"accuracy\": train_accuracy, \"logloss\": train_logloss},\n",
    "        \"val\": {\"accuracy\": val_accuracy, \"logloss\": val_logloss},\n",
    "        \"test\": {\"accuracy\": test_accuracy, \"logloss\": test_logloss},\n",
    "        \"epochs_trained\": epochs_trained,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"dataset_label\": DATASET_LABEL,\n",
    "    }\n",
    "    if class_weights is not None:\n",
    "        metrics_payload[\"class_weights\"] = class_weights\n",
    "\n",
    "    with open(baseline_dir / \"metrics.json\", \"w\") as fp:\n",
    "        json.dump(to_serializable(metrics_payload), fp, indent=2)\n",
    "\n",
    "    baseline_results.append({\n",
    "        \"baseline\": baseline_name,\n",
    "        \"trainer\": trainer,\n",
    "        \"description\": spec[\"description\"],\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"val_logloss\": val_logloss,\n",
    "        \"test_logloss\": test_logloss,\n",
    "        \"epochs\": epochs_trained,\n",
    "        \"dataset_label\": DATASET_LABEL,\n",
    "    })\n",
    "\n",
    "    run_log_entries.append({\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"baseline\": baseline_name,\n",
    "        \"trainer\": trainer,\n",
    "        \"feature_view\": spec[\"description\"],\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"train_loss\": train_logloss,\n",
    "        \"val_loss\": val_logloss,\n",
    "        \"test_loss\": test_logloss,\n",
    "        \"val_logloss\": val_logloss,\n",
    "        \"test_logloss\": test_logloss,\n",
    "        \"epochs_trained\": epochs_trained,\n",
    "        \"seasons\": \"|\".join(SEASONS),\n",
    "        \"dataset_label\": DATASET_LABEL or \"\",\n",
    "        \"notes\": \"\",\n",
    "    })\n",
    "\n",
    "baseline_summary_df = pd.DataFrame(baseline_results).sort_values(\"val_accuracy\", ascending=False)\n",
    "display(baseline_summary_df)\n",
    "\n",
    "if run_log_entries:\n",
    "    run_log_df = pd.DataFrame(run_log_entries).reindex(columns=RUN_LOG_COLUMNS)\n",
    "    existing_log_df = load_run_log(RUN_LOG_PATH)\n",
    "    combined_log_df = pd.concat([existing_log_df, run_log_df], ignore_index=True)\n",
    "    combined_log_df.to_csv(RUN_LOG_PATH, index=False)\n",
    "    display(run_log_df[[\"baseline\", \"trainer\", \"dataset_label\", \"val_accuracy\", \"test_accuracy\", \"epochs_trained\"]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ac796",
   "metadata": {},
   "source": [
    "## Historical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474841fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:49:04.356418Z",
     "iopub.status.busy": "2025-10-24T19:49:04.356088Z",
     "iopub.status.idle": "2025-10-24T19:49:04.875142Z",
     "shell.execute_reply": "2025-10-24T19:49:04.87422Z",
     "shell.execute_reply.started": "2025-10-24T19:49:04.35639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Historical comparison versus prior runs\n",
    "if RUN_LOG_PATH.exists():\n",
    "    history_df = load_run_log(RUN_LOG_PATH)\n",
    "    existing_run_ids = {\n",
    "        p.name.replace(\"run_\", \"\")\n",
    "        for p in EXPERIMENT_ROOT.glob(\"run_*\")\n",
    "        if p.is_dir()\n",
    "    }\n",
    "    if existing_run_ids:\n",
    "        history_df = history_df[history_df[\"run_id\"].isin(existing_run_ids)].copy()\n",
    "    else:\n",
    "        history_df = history_df.iloc[0:0]\n",
    "    history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], errors=\"coerce\")\n",
    "    history_df.sort_values(\"timestamp\", inplace=True)\n",
    "\n",
    "    if history_df.empty:\n",
    "        print(\"No completed runs with artefacts found. Generate a new run and rerun this cell.\")\n",
    "    else:\n",
    "        unlabeled_mask = history_df[\"dataset_label\"].astype(str).str.strip() == \"\"\n",
    "        unlabeled_df = history_df[unlabeled_mask]\n",
    "\n",
    "        if not unlabeled_df.empty:\n",
    "            print(\"Some historical runs are missing dataset labels. Provide a label for each data point below and click 'Apply labels', then rerun this cell to render the chart.\")\n",
    "            label_inputs = {}\n",
    "            widgets_list = []\n",
    "            for row_index, row in unlabeled_df.iterrows():\n",
    "                default_value = DATASET_LABEL or \"\"\n",
    "                descriptor = f\"{row[\"baseline\"]} â€” {row[\"run_id\"]}\"\n",
    "                text_widget = widgets.Text(\n",
    "                    value=default_value,\n",
    "                    description=descriptor,\n",
    "                    placeholder=\"Enter dataset name\",\n",
    "                    layout=widgets.Layout(width=\"70%\"),\n",
    "                )\n",
    "                label_inputs[row_index] = text_widget\n",
    "                widgets_list.append(text_widget)\n",
    "\n",
    "            apply_button = widgets.Button(description=\"Apply labels\", button_style=\"primary\")\n",
    "            status_html = widgets.HTML()\n",
    "\n",
    "            def _apply_labels(_):\n",
    "                for row_index, widget_box in label_inputs.items():\n",
    "                    history_df.at[row_index, \"dataset_label\"] = widget_box.value.strip()\n",
    "                history_df[RUN_LOG_COLUMNS].to_csv(RUN_LOG_PATH, index=False)\n",
    "                status_html.value = \"<span style='color:green;'>Dataset labels saved. Rerun this cell to update the comparison chart.</span>\"\n",
    "\n",
    "            apply_button.on_click(_apply_labels)\n",
    "            display(widgets.VBox(widgets_list + [apply_button, status_html]))\n",
    "        else:\n",
    "            latest_df = history_df[history_df[\"run_id\"] == RUN_ID]\n",
    "            best_df = history_df.sort_values(\"val_accuracy\", ascending=False).drop_duplicates(\"baseline\")\n",
    "            comparison = latest_df.merge(\n",
    "                best_df[[\"baseline\", \"val_accuracy\", \"test_accuracy\", \"run_id\", \"dataset_label\"]],\n",
    "                on=\"baseline\",\n",
    "                suffixes=(\"_current\", \"_best\"),\n",
    "            )\n",
    "\n",
    "            print(\"Most recent run vs historical best (by validation accuracy):\")\n",
    "            if not comparison.empty:\n",
    "                display(comparison[[\n",
    "                    \"baseline\",\n",
    "                    \"dataset_label_current\",\n",
    "                    \"dataset_label_best\",\n",
    "                    \"val_accuracy_current\",\n",
    "                    \"val_accuracy_best\",\n",
    "                    \"test_accuracy_current\",\n",
    "                    \"test_accuracy_best\",\n",
    "                    \"run_id_best\",\n",
    "                ]])\n",
    "            else:\n",
    "                print(\"No runs for the current run ID yet.\")\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            for baseline in history_df[\"baseline\"].unique():\n",
    "                subset = history_df[history_df[\"baseline\"] == baseline].dropna(subset=[\"timestamp\"])\n",
    "                if subset.empty:\n",
    "                    continue\n",
    "                ax.plot(subset[\"timestamp\"], subset[\"val_accuracy\"], marker=\"o\", label=baseline)\n",
    "                for row in subset.itertuples():\n",
    "                    label_text = getattr(row, \"dataset_label\", \"\").strip()\n",
    "                    if label_text:\n",
    "                        ax.annotate(\n",
    "                            label_text,\n",
    "                            (row.timestamp, row.val_accuracy),\n",
    "                            textcoords=\"offset points\",\n",
    "                            xytext=(0, 6),\n",
    "                            ha=\"center\",\n",
    "                            fontsize=8,\n",
    "                            color=\"dimgray\",\n",
    "                        )\n",
    "            ax.set_title(\"Validation accuracy trajectory by baseline\")\n",
    "            ax.set_ylabel(\"Validation accuracy\")\n",
    "            ax.set_xlabel(\"Run timestamp\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "            fig.autofmt_xdate()\n",
    "            fig.tight_layout()\n",
    "            plot_path = MODEL_ARTIFACT_DIR / \"validation_accuracy_history.png\"\n",
    "            fig.savefig(plot_path, dpi=160)\n",
    "            plt.show()\n",
    "            print(f\"Saved validation accuracy history plot to {plot_path}\")\n",
    "else:\n",
    "    print(\"Run history file not found â€” this will be created after the first successful training run.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ba7af",
   "metadata": {},
   "source": [
    "\n",
    "## Next Steps\n",
    "- Extend feature engineering with financial and odds datasets described in the README, maintaining the logging conventions used here.\n",
    "- Layer on attribution analysis (approximate Shapley, LOO) by loading saved `model.keras` files and pairing them with cached train/validation datasets.\n",
    "- Use the persisted `baseline_run_history.csv` to drive Kaggle dashboard widgets or automated alerts when new dataset versions shift model behaviour.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8565895,
     "sourceId": 13491323,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "BeautGame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}