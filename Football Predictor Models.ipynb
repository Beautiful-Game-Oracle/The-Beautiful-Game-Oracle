{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad91f68b",
   "metadata": {},
   "source": [
    "\n",
    "# The Beautiful Game Oracle — Baseline TensorFlow Suite\n",
    "\n",
    "This notebook prepares and compares three TensorFlow/Keras baseline models for predicting English Premier League match outcomes using data pulled directly from the Understat API. It aligns with the project charter in `README.md` and the agent directives in `AGENTS.md`, emphasising reproducible experiments, attribution readiness, and run tracking for longitudinal comparisons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2623a92",
   "metadata": {},
   "source": [
    "\n",
    "## Workflow Overview\n",
    "- Fetch and cache historical EPL match data from Understat for configurable seasons.\n",
    "- Engineer team form, momentum, and market-derived features compatible with TensorFlow pipelines.\n",
    "- Train three complementary baselines (performance-form dense net, momentum interaction network, forecast calibrator) and save artefacts for reuse.\n",
    "- Persist metrics and artefacts per run, append to a cumulative history log, and generate comparison visuals versus prior runs.\n",
    "- Prepare the infrastructure needed for downstream attribution work (Shapley/LOO) by keeping models and datasets aligned with saved run metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e7ee7",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827c5b4",
   "metadata": {},
   "source": [
    "## Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experiment configuration\n",
    "PROJECT_NAME = \"The-Beautiful-Game-Oracle\"\n",
    "LEAGUE = \"EPL\"\n",
    "SEASONS = [\"2023\", \"2022\", \"2021\", \"2020\"]  # extend or adjust as needed\n",
    "ROLLING_WINDOW = 5\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "REFRESH_DATA = False  # set True to refetch from Understat\n",
    "RUN_ID = datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Filesystem locations (compatible with Kaggle + local use)\n",
    "if Path(\"/kaggle\").exists():\n",
    "    BASE_WORKING_DIR = Path(\"/kaggle/working\")\n",
    "else:\n",
    "    BASE_WORKING_DIR = Path(\"./artifacts\")\n",
    "\n",
    "EXPERIMENT_ROOT = BASE_WORKING_DIR / \"experiments\"\n",
    "CACHE_DIR = EXPERIMENT_ROOT / \"understat_cache\"\n",
    "MODEL_ARTIFACT_DIR = EXPERIMENT_ROOT / f\"run_{RUN_ID}\"\n",
    "RUN_LOG_PATH = EXPERIMENT_ROOT / \"baseline_run_history.csv\"\n",
    "\n",
    "for path in [EXPERIMENT_ROOT, CACHE_DIR, MODEL_ARTIFACT_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project: {PROJECT_NAME}\")\n",
    "print(f\"Seasons loaded: {SEASONS}\")\n",
    "print(f\"Working dir: {BASE_WORKING_DIR.resolve()}\")\n",
    "print(f\"Current run artefacts: {MODEL_ARTIFACT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc6855",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Understat data retrieval helpers\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; The-Beautiful-Game-Oracle/1.0)\",\n",
    "}\n",
    "\n",
    "MATCHES_PATTERN = re.compile(r\"var\\s+datesData\\s*=\\s*JSON.parse\\('(.+?)'\\)\")\n",
    "\n",
    "\n",
    "def fetch_understat_dates(league: str, season: str, *, refresh: bool = False) -> list:\n",
    "    '''Download Understat league matches for a season, with on-disk caching.'''\n",
    "    cache_path = CACHE_DIR / f\"{league}_{season}_dates.json\"\n",
    "    if cache_path.exists() and not refresh:\n",
    "        try:\n",
    "            return json.loads(cache_path.read_text())\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # fall back to refetching\n",
    "\n",
    "    url = f\"https://understat.com/league/{league}/{season}\"\n",
    "    response = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    match = MATCHES_PATTERN.search(response.text)\n",
    "    if not match:\n",
    "        raise RuntimeError(f\"datesData block not found for {league} {season}\")\n",
    "    decoded = match.group(1).encode(\"utf-8\").decode(\"unicode_escape\")\n",
    "    data = json.loads(decoded)\n",
    "    cache_path.write_text(json.dumps(data))\n",
    "    time.sleep(0.5)  # be gentle with the source\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_matches_for_seasons(league: str, seasons: list[str], refresh: bool = False) -> dict[str, list]:\n",
    "    payload = {}\n",
    "    for season in seasons:\n",
    "        payload[season] = fetch_understat_dates(league, season, refresh=refresh)\n",
    "        print(f\"Loaded {len(payload[season])} fixtures for {league} {season}\")\n",
    "    return payload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0a709",
   "metadata": {},
   "source": [
    "## Feature Engineering Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84bb6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature engineering utilities\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "CLASS_LABELS = [\"Home Win\", \"Draw\", \"Away Win\"]\n",
    "\n",
    "\n",
    "def build_match_dataframe(matches_by_season: Dict[str, List[dict]]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for season, matches in matches_by_season.items():\n",
    "        for match in matches:\n",
    "            if not match.get(\"isResult\"):\n",
    "                continue  # skip fixtures without a final result\n",
    "            rows.append({\n",
    "                \"match_id\": int(match[\"id\"]),\n",
    "                \"season\": season,\n",
    "                \"match_date\": pd.to_datetime(match[\"datetime\"], format=\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"home_team\": match[\"h\"][\"title\"],\n",
    "                \"away_team\": match[\"a\"][\"title\"],\n",
    "                \"home_goals\": int(match[\"goals\"][\"h\"]),\n",
    "                \"away_goals\": int(match[\"goals\"][\"a\"]),\n",
    "                \"home_xg\": float(match[\"xG\"][\"h\"]),\n",
    "                \"away_xg\": float(match[\"xG\"][\"a\"]),\n",
    "                \"home_prob_win\": float(match[\"forecast\"][\"w\"]),\n",
    "                \"draw_prob\": float(match[\"forecast\"][\"d\"]),\n",
    "                \"away_prob_win\": float(match[\"forecast\"][\"l\"]),\n",
    "            })\n",
    "    frame = pd.DataFrame(rows).sort_values(\"match_date\").reset_index(drop=True)\n",
    "    print(f\"Prepared {len(frame)} completed fixtures across seasons {sorted(matches_by_season.keys())}\")\n",
    "    return frame\n",
    "\n",
    "\n",
    "def build_team_match_rows(match_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for row in match_df.itertuples(index=False):\n",
    "        rows.append({\n",
    "            \"match_id\": row.match_id,\n",
    "            \"season\": row.season,\n",
    "            \"match_date\": row.match_date,\n",
    "            \"team\": row.home_team,\n",
    "            \"opponent\": row.away_team,\n",
    "            \"is_home\": 1,\n",
    "            \"goals_for\": row.home_goals,\n",
    "            \"goals_against\": row.away_goals,\n",
    "            \"xg_for\": row.home_xg,\n",
    "            \"xg_against\": row.away_xg,\n",
    "            \"prob_win\": row.home_prob_win,\n",
    "            \"prob_loss\": row.away_prob_win,\n",
    "            \"draw_prob\": row.draw_prob,\n",
    "        })\n",
    "        rows.append({\n",
    "            \"match_id\": row.match_id,\n",
    "            \"season\": row.season,\n",
    "            \"match_date\": row.match_date,\n",
    "            \"team\": row.away_team,\n",
    "            \"opponent\": row.home_team,\n",
    "            \"is_home\": 0,\n",
    "            \"goals_for\": row.away_goals,\n",
    "            \"goals_against\": row.home_goals,\n",
    "            \"xg_for\": row.away_xg,\n",
    "            \"xg_against\": row.home_xg,\n",
    "            \"prob_win\": row.away_prob_win,\n",
    "            \"prob_loss\": row.home_prob_win,\n",
    "            \"draw_prob\": row.draw_prob,\n",
    "        })\n",
    "    team_df = pd.DataFrame(rows).sort_values(\"match_date\").reset_index(drop=True)\n",
    "    team_df[\"goal_diff\"] = team_df[\"goals_for\"] - team_df[\"goals_against\"]\n",
    "    team_df[\"xg_diff\"] = team_df[\"xg_for\"] - team_df[\"xg_against\"]\n",
    "    return team_df\n",
    "\n",
    "\n",
    "def add_form_features(team_df: pd.DataFrame, window: int = 5) -> pd.DataFrame:\n",
    "    feature_cols = [\n",
    "        \"goals_for\",\n",
    "        \"goals_against\",\n",
    "        \"goal_diff\",\n",
    "        \"xg_for\",\n",
    "        \"xg_against\",\n",
    "        \"xg_diff\",\n",
    "        \"prob_win\",\n",
    "    ]\n",
    "\n",
    "    def enrich(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for col in feature_cols:\n",
    "            shifted = g[col].shift(1)\n",
    "            g[f\"form_{col}_mean\"] = shifted.rolling(window, min_periods=1).mean().fillna(0)\n",
    "            g[f\"form_{col}_std\"] = shifted.rolling(window, min_periods=1).std().fillna(0)\n",
    "            g[f\"form_{col}_last\"] = shifted.fillna(0)\n",
    "        return g\n",
    "\n",
    "    enriched = (\n",
    "        team_df.sort_values(\"match_date\")\n",
    "        .groupby(\"team\", group_keys=False)\n",
    "        .apply(enrich)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return enriched\n",
    "\n",
    "\n",
    "def assemble_match_features(team_form_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    keep_cols = [\"match_id\", \"season\", \"match_date\", \"home_team\", \"away_team\"]\n",
    "\n",
    "    home = team_form_df[team_form_df[\"is_home\"] == 1].copy()\n",
    "    away = team_form_df[team_form_df[\"is_home\"] == 0].copy()\n",
    "\n",
    "    home.rename(columns={\"team\": \"home_team\", \"opponent\": \"away_team\"}, inplace=True)\n",
    "    away.rename(columns={\"team\": \"away_team\", \"opponent\": \"home_team\"}, inplace=True)\n",
    "\n",
    "    def prefix_except(df: pd.DataFrame, prefix: str, exclude: List[str]) -> pd.DataFrame:\n",
    "        rename_map = {col: f\"{prefix}{col}\" for col in df.columns if col not in exclude}\n",
    "        return df.rename(columns=rename_map)\n",
    "\n",
    "    home_prefixed = prefix_except(home, \"home_\", keep_cols)\n",
    "    away_prefixed = prefix_except(away, \"away_\", keep_cols)\n",
    "\n",
    "    merged = home_prefixed.merge(\n",
    "        away_prefixed,\n",
    "        on=[\"match_id\", \"season\", \"match_date\", \"home_team\", \"away_team\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"\", \"_dup\"),\n",
    "    )\n",
    "\n",
    "    if \"home_draw_prob\" in merged.columns:\n",
    "        merged.rename(columns={\"home_draw_prob\": \"match_draw_prob\"}, inplace=True)\n",
    "    if \"away_draw_prob\" in merged.columns:\n",
    "        merged.drop(columns=[\"away_draw_prob\"], inplace=True)\n",
    "\n",
    "    merged[\"target\"] = np.select(\n",
    "        [\n",
    "            merged[\"home_goals_for\"] > merged[\"away_goals_for\"],\n",
    "            merged[\"home_goals_for\"] == merged[\"away_goals_for\"],\n",
    "        ],\n",
    "        [0, 1],\n",
    "        default=2,\n",
    "    )\n",
    "\n",
    "    merged.sort_values(\"match_date\", inplace=True)\n",
    "    merged.reset_index(drop=True, inplace=True)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def build_feature_deltas(match_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = match_df.copy()\n",
    "    df[\"form_goal_diff_delta\"] = df[\"home_form_goal_diff_mean\"] - df[\"away_form_goal_diff_mean\"]\n",
    "    df[\"form_xg_diff_delta\"] = df[\"home_form_xg_diff_mean\"] - df[\"away_form_xg_diff_mean\"]\n",
    "    df[\"form_prob_win_delta\"] = df[\"home_form_prob_win_mean\"] - df[\"away_form_prob_win_mean\"]\n",
    "    df[\"form_goal_last_delta\"] = df[\"home_form_goal_diff_last\"] - df[\"away_form_goal_diff_last\"]\n",
    "    df[\"form_xg_last_delta\"] = df[\"home_form_xg_diff_last\"] - df[\"away_form_xg_diff_last\"]\n",
    "    df[\"prob_edge\"] = df[\"home_prob_win\"] - df[\"away_prob_win\"]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3134b4b",
   "metadata": {},
   "source": [
    "## Dataset Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and prepare datasets\n",
    "matches_payload = load_matches_for_seasons(LEAGUE, SEASONS, refresh=REFRESH_DATA)\n",
    "match_df = build_match_dataframe(matches_payload)\n",
    "team_df = build_team_match_rows(match_df)\n",
    "team_form_df = add_form_features(team_df, window=ROLLING_WINDOW)\n",
    "match_features_df = assemble_match_features(team_form_df)\n",
    "match_features_df = build_feature_deltas(match_features_df)\n",
    "\n",
    "feature_columns = [col for col in match_features_df.columns if col.startswith((\"home_\", \"away_\", \"form_\", \"match_\", \"prob_\"))]\n",
    "match_features_df[feature_columns] = match_features_df[feature_columns].fillna(0)\n",
    "\n",
    "print(f\"Final feature table: {match_features_df.shape[0]} matches x {len(feature_columns)} engineered columns\")\n",
    "display(match_features_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77861e6b",
   "metadata": {},
   "source": [
    "## Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beeb7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chronological train/validation/test split\n",
    "sorted_df = match_features_df.sort_values(\"match_date\").reset_index(drop=True)\n",
    "num_matches = len(sorted_df)\n",
    "train_end = int(num_matches * 0.6)\n",
    "val_end = int(num_matches * 0.8)\n",
    "\n",
    "split_indices = {\n",
    "    \"train\": sorted_df.index[:train_end],\n",
    "    \"val\": sorted_df.index[train_end:val_end],\n",
    "    \"test\": sorted_df.index[val_end:],\n",
    "}\n",
    "\n",
    "print({k: len(v) for k, v in split_indices.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5d2dd",
   "metadata": {},
   "source": [
    "## Feature Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature sets aligned with the three data perspectives outlined in README.md\n",
    "PERFORMANCE_FEATURES = [\n",
    "    \"home_form_goals_for_mean\",\n",
    "    \"home_form_goals_against_mean\",\n",
    "    \"home_form_goal_diff_mean\",\n",
    "    \"home_form_xg_for_mean\",\n",
    "    \"home_form_xg_against_mean\",\n",
    "    \"home_form_xg_diff_mean\",\n",
    "    \"away_form_goals_for_mean\",\n",
    "    \"away_form_goals_against_mean\",\n",
    "    \"away_form_goal_diff_mean\",\n",
    "    \"away_form_xg_for_mean\",\n",
    "    \"away_form_xg_against_mean\",\n",
    "    \"away_form_xg_diff_mean\",\n",
    "    \"form_goal_diff_delta\",\n",
    "    \"form_xg_diff_delta\",\n",
    "]\n",
    "\n",
    "MOMENTUM_FEATURES = [\n",
    "    \"home_form_goal_diff_last\",\n",
    "    \"home_form_xg_diff_last\",\n",
    "    \"home_form_goal_diff_std\",\n",
    "    \"away_form_goal_diff_last\",\n",
    "    \"away_form_xg_diff_last\",\n",
    "    \"away_form_goal_diff_std\",\n",
    "    \"form_goal_last_delta\",\n",
    "    \"form_xg_last_delta\",\n",
    "    \"form_goal_diff_delta\",\n",
    "    \"form_xg_diff_delta\",\n",
    "    \"form_prob_win_delta\",\n",
    "]\n",
    "\n",
    "MARKET_FEATURES = [\n",
    "    \"home_prob_win\",\n",
    "    \"home_prob_loss\",\n",
    "    \"away_prob_win\",\n",
    "    \"away_prob_loss\",\n",
    "    \"match_draw_prob\",\n",
    "    \"home_form_prob_win_mean\",\n",
    "    \"away_form_prob_win_mean\",\n",
    "    \"form_prob_win_delta\",\n",
    "    \"prob_edge\",\n",
    "]\n",
    "\n",
    "FEATURE_SETS = {\n",
    "    \"performance_dense\": {\n",
    "        \"description\": \"Performance-based dense network using rolling xG/goals form aggregates\",\n",
    "        \"feature_cols\": PERFORMANCE_FEATURES,\n",
    "    },\n",
    "    \"momentum_interaction\": {\n",
    "        \"description\": \"Momentum interaction MLP on recent differential form signals\",\n",
    "        \"feature_cols\": MOMENTUM_FEATURES,\n",
    "    },\n",
    "    \"forecast_calibrator\": {\n",
    "        \"description\": \"Understat forecast calibrator focusing on market-style probabilities\",\n",
    "        \"feature_cols\": MARKET_FEATURES,\n",
    "    },\n",
    "}\n",
    "\n",
    "print({name: len(spec[\"feature_cols\"]) for name, spec in FEATURE_SETS.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8548b",
   "metadata": {},
   "source": [
    "## TensorFlow Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset constructors\n",
    "\n",
    "def build_xy(df: pd.DataFrame, feature_cols: list[str], indices: pd.Index) -> tuple[np.ndarray, np.ndarray]:\n",
    "    X = df.loc[indices, feature_cols].astype(np.float32).to_numpy()\n",
    "    y = df.loc[indices, \"target\"].astype(np.int32).to_numpy()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_dataset(X: np.ndarray, y: np.ndarray, *, training: bool) -> tf.data.Dataset:\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=len(X), seed=SEED, reshuffle_each_iteration=True)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "def compute_class_weights(y: np.ndarray) -> dict[int, float]:\n",
    "    counts = np.bincount(y, minlength=len(CLASS_LABELS))\n",
    "    total = float(len(y))\n",
    "    weights = {cls: total / (len(CLASS_LABELS) * count) for cls, count in enumerate(counts) if count > 0}\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae37026",
   "metadata": {},
   "source": [
    "## Baseline Model Builders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4594c2",
   "metadata": {},
   "source": [
    "### Performance Dense Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62120879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_performance_model(input_dim: int) -> keras.Model:\n",
    "    inputs = keras.Input(shape=(input_dim,), name=\"performance_features\")\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "    x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dropout(0.15)(x)\n",
    "    outputs = keras.layers.Dense(len(CLASS_LABELS), activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"performance_dense\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32a102",
   "metadata": {},
   "source": [
    "### Momentum Interaction Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9bf86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_momentum_model(input_dim: int) -> keras.Model:\n",
    "    inputs = keras.Input(shape=(input_dim,), name=\"momentum_features\")\n",
    "    x = keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(1e-4))(inputs)\n",
    "    x = keras.layers.Dropout(0.35)(x)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(len(CLASS_LABELS), activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"momentum_interaction\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=7e-4),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6e9e3",
   "metadata": {},
   "source": [
    "### Forecast Calibrator Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_forecast_model(input_dim: int) -> keras.Model:\n",
    "    inputs = keras.Input(shape=(input_dim,), name=\"forecast_features\")\n",
    "    x = keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
    "    x = keras.layers.Dropout(0.1)(x)\n",
    "    x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(len(CLASS_LABELS), activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"forecast_calibrator\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb0f997",
   "metadata": {},
   "source": [
    "### Baseline Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5644dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_BUILDERS = {\n",
    "    \"performance_dense\": build_performance_model,\n",
    "    \"momentum_interaction\": build_momentum_model,\n",
    "    \"forecast_calibrator\": build_forecast_model,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd01ac",
   "metadata": {},
   "source": [
    "## Training and Run Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e1353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop with experiment logging\n",
    "sce = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "baseline_results = []\n",
    "run_log_entries = []\n",
    "\n",
    "for baseline_name, spec in FEATURE_SETS.items():\n",
    "    feature_cols = spec[\"feature_cols\"]\n",
    "    builder = BASELINE_BUILDERS[baseline_name]\n",
    "\n",
    "    X_train, y_train = build_xy(sorted_df, feature_cols, split_indices[\"train\"])\n",
    "    X_val, y_val = build_xy(sorted_df, feature_cols, split_indices[\"val\"])\n",
    "    X_test, y_test = build_xy(sorted_df, feature_cols, split_indices[\"test\"])\n",
    "\n",
    "    train_ds = make_dataset(X_train, y_train, training=True)\n",
    "    val_ds = make_dataset(X_val, y_val, training=False)\n",
    "    test_ds = make_dataset(X_test, y_test, training=False)\n",
    "\n",
    "    model = builder(len(feature_cols))\n",
    "    class_weights = compute_class_weights(y_train)\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5),\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    train_metrics = model.evaluate(train_ds, verbose=0)\n",
    "    val_metrics = model.evaluate(val_ds, verbose=0)\n",
    "    test_metrics = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "    val_probs = model.predict(val_ds, verbose=0)\n",
    "    test_probs = model.predict(test_ds, verbose=0)\n",
    "    val_preds = np.argmax(val_probs, axis=1)\n",
    "    test_preds = np.argmax(test_probs, axis=1)\n",
    "\n",
    "    val_accuracy = float(np.mean(val_preds == y_val))\n",
    "    test_accuracy = float(np.mean(test_preds == y_test))\n",
    "    val_logloss = float(sce(y_val, val_probs).numpy())\n",
    "    test_logloss = float(sce(y_test, test_probs).numpy())\n",
    "\n",
    "    confusion = tf.math.confusion_matrix(y_test, test_preds, num_classes=len(CLASS_LABELS)).numpy()\n",
    "\n",
    "    baseline_dir = MODEL_ARTIFACT_DIR / baseline_name\n",
    "    baseline_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save(baseline_dir / \"model.keras\", include_optimizer=False)\n",
    "    with open(baseline_dir / \"history.json\", \"w\") as fp:\n",
    "        json.dump(history.history, fp, indent=2)\n",
    "    with open(baseline_dir / \"metrics.json\", \"w\") as fp:\n",
    "        json.dump({\n",
    "            \"train\": {\"loss\": float(train_metrics[0]), \"accuracy\": float(train_metrics[1])},\n",
    "            \"val\": {\"loss\": float(val_metrics[0]), \"accuracy\": float(val_metrics[1])},\n",
    "            \"test\": {\"loss\": float(test_metrics[0]), \"accuracy\": float(test_metrics[1])},\n",
    "            \"val_logloss\": val_logloss,\n",
    "            \"test_logloss\": test_logloss,\n",
    "            \"class_weights\": class_weights,\n",
    "            \"feature_cols\": feature_cols,\n",
    "        }, fp, indent=2)\n",
    "\n",
    "    eval_rows = sorted_df.loc[split_indices[\"test\"], [\"match_id\", \"season\", \"match_date\", \"home_team\", \"away_team\", \"target\"]].copy()\n",
    "    eval_rows[\"predicted\"] = test_preds\n",
    "    eval_rows[\"predicted_label\"] = eval_rows[\"predicted\"].map(dict(enumerate(CLASS_LABELS)))\n",
    "    eval_rows[\"target_label\"] = eval_rows[\"target\"].map(dict(enumerate(CLASS_LABELS)))\n",
    "    eval_rows[\"home_prob\"] = test_probs[:, 0]\n",
    "    eval_rows[\"draw_prob\"] = test_probs[:, 1]\n",
    "    eval_rows[\"away_prob\"] = test_probs[:, 2]\n",
    "    eval_rows.to_csv(baseline_dir / \"test_predictions.csv\", index=False)\n",
    "\n",
    "    def save_confusion_heatmap(cm: np.ndarray, labels: list[str], path: Path, title: str) -> None:\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        im = ax.imshow(cm, cmap=\"Blues\")\n",
    "        ax.set_xticks(range(len(labels)))\n",
    "        ax.set_yticks(range(len(labels)))\n",
    "        ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "        ax.set_yticklabels(labels)\n",
    "        for i in range(len(labels)):\n",
    "            for j in range(len(labels)):\n",
    "                ax.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"Actual\")\n",
    "        ax.set_title(title)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(path, dpi=160)\n",
    "        plt.close(fig)\n",
    "\n",
    "    save_confusion_heatmap(confusion, CLASS_LABELS, baseline_dir / \"confusion_matrix.png\", f\"{baseline_name} — Test Confusion\")\n",
    "\n",
    "    baseline_results.append({\n",
    "        \"baseline\": baseline_name,\n",
    "        \"description\": spec[\"description\"],\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"val_logloss\": val_logloss,\n",
    "        \"test_logloss\": test_logloss,\n",
    "        \"epochs\": len(history.history[\"loss\"]),\n",
    "    })\n",
    "\n",
    "    run_log_entries.append({\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"baseline\": baseline_name,\n",
    "        \"feature_view\": spec[\"description\"],\n",
    "        \"train_accuracy\": float(train_metrics[1]),\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"val_loss\": float(val_metrics[0]),\n",
    "        \"test_loss\": float(test_metrics[0]),\n",
    "        \"val_logloss\": val_logloss,\n",
    "        \"test_logloss\": test_logloss,\n",
    "        \"epochs_trained\": len(history.history[\"loss\"]),\n",
    "        \"seasons\": \"|\".join(SEASONS),\n",
    "        \"notes\": \"\",\n",
    "    })\n",
    "\n",
    "baseline_summary_df = pd.DataFrame(baseline_results).sort_values(\"val_accuracy\", ascending=False)\n",
    "display(baseline_summary_df)\n",
    "\n",
    "if run_log_entries:\n",
    "    run_log_df = pd.DataFrame(run_log_entries)\n",
    "    if RUN_LOG_PATH.exists():\n",
    "        run_log_df.to_csv(RUN_LOG_PATH, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        run_log_df.to_csv(RUN_LOG_PATH, index=False)\n",
    "    display(run_log_df[[\"baseline\", \"val_accuracy\", \"test_accuracy\", \"epochs_trained\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ac796",
   "metadata": {},
   "source": [
    "## Historical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474841fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Historical comparison versus prior runs\n",
    "if RUN_LOG_PATH.exists():\n",
    "    history_df = pd.read_csv(RUN_LOG_PATH)\n",
    "    history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], errors=\"coerce\")\n",
    "    history_df.sort_values(\"timestamp\", inplace=True)\n",
    "\n",
    "    latest_df = history_df[history_df[\"run_id\"] == RUN_ID]\n",
    "    best_df = history_df.sort_values(\"val_accuracy\", ascending=False).drop_duplicates(\"baseline\")\n",
    "    comparison = latest_df.merge(\n",
    "        best_df[[\"baseline\", \"val_accuracy\", \"test_accuracy\", \"run_id\"]],\n",
    "        on=\"baseline\",\n",
    "        suffixes=(\"_current\", \"_best\"),\n",
    "    )\n",
    "\n",
    "    print(\"Most recent run vs historical best (by validation accuracy):\")\n",
    "    display(comparison[[\n",
    "        \"baseline\",\n",
    "        \"val_accuracy_current\",\n",
    "        \"val_accuracy_best\",\n",
    "        \"test_accuracy_current\",\n",
    "        \"test_accuracy_best\",\n",
    "        \"run_id_best\",\n",
    "    ]])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    for baseline in history_df[\"baseline\"].unique():\n",
    "        subset = history_df[history_df[\"baseline\"] == baseline]\n",
    "        ax.plot(subset[\"timestamp\"], subset[\"val_accuracy\"], marker=\"o\", label=baseline)\n",
    "    ax.set_title(\"Validation accuracy trajectory by baseline\")\n",
    "    ax.set_ylabel(\"Validation accuracy\")\n",
    "    ax.set_xlabel(\"Run timestamp\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout()\n",
    "    plot_path = MODEL_ARTIFACT_DIR / \"validation_accuracy_history.png\"\n",
    "    fig.savefig(plot_path, dpi=160)\n",
    "    plt.show()\n",
    "    print(f\"Saved validation accuracy history plot to {plot_path}\")\n",
    "else:\n",
    "    print(\"Run history file not found — this will be created after the first successful training run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ba7af",
   "metadata": {},
   "source": [
    "\n",
    "## Next Steps\n",
    "- Extend feature engineering with financial and odds datasets described in the README, maintaining the logging conventions used here.\n",
    "- Layer on attribution analysis (approximate Shapley, LOO) by loading saved `model.keras` files and pairing them with cached train/validation datasets.\n",
    "- Use the persisted `baseline_run_history.csv` to drive Kaggle dashboard widgets or automated alerts when new dataset versions shift model behaviour.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 63,
     "sourceId": 589,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 712,
     "sourceId": 1336,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1556509,
     "sourceId": 2564419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4305,
     "sourceId": 13048059,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
